{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2_Perceiver_ImageClassn.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw-X7NuRNqOJ"
      },
      "source": [
        "# **Perceiver model implementation for image classification**\n",
        "\n",
        "* Dataset: CIFAR-100 dataset - \n",
        "  * The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class.\n",
        "  * CIFAR-100 dataset is just like the CIFAR-10, except that it has 100 classes containing 600 images each. \n",
        "  * There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. \n",
        "  * Each image comes with a \"fine\" label (the class to which it belongs) and a \"coarse\" label (the superclass to which it belongs).\n",
        "* The Perceiver model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs.\n",
        "* The perception models used in deep learning are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models\n",
        "\n",
        "\n",
        "## **References:**\n",
        "* https://keras.io/examples/vision/perceiver_image_classification/\n",
        "* https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/perceiver_image_classification.ipynb\n",
        "* https://arxiv.org/pdf/2103.03206.pdf\n",
        "* https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDs7VSyjcGnb"
      },
      "source": [
        "## **Installables**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhwZ0dLCb4Te",
        "outputId": "4391d110-3be4-4c1f-cffc-d23b2da08361"
      },
      "source": [
        "pip install -U tensorflow-addons"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.14.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCnZ0ST4cQMa"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhjICvssPgz1"
      },
      "source": [
        "The Perceiver model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4j1UMrXcWej"
      },
      "source": [
        "## **Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGvUraYxcY8B",
        "outputId": "3c4f58fb-91d8-45ad-e0eb-edd8096bdd60"
      },
      "source": [
        "num_classes = 100\n",
        "input_shape = (32, 32, 3)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 11s 0us/step\n",
            "169017344/169001437 [==============================] - 11s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
            "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNhxf4-Bci7D"
      },
      "source": [
        "## **Hyperparameter configuration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Io8IR0z5clxg",
        "outputId": "02a2f3ec-4aea-4864-e428-502cb3b5ca46"
      },
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 64\n",
        "num_epochs = 50\n",
        "dropout_rate = 0.2\n",
        "## resizing input images to 64 size.\n",
        "image_size = 64 \n",
        "## Represents size of the patches to be extracted from the input images. \n",
        "patch_size = 2  \n",
        "## Size of the data array.\n",
        "num_patches = (image_size // patch_size) ** 2  \n",
        "## Size of the latent array.\n",
        "latent_dim = 256  \n",
        "## Embedding size of each element in the data and latent arrays.\n",
        "projection_dim = 256  \n",
        "## Number of Transformer heads.\n",
        "num_heads = 8  \n",
        "## Size of the Transformer Feedforward network.\n",
        "ffn_units = [\n",
        "    projection_dim,\n",
        "    projection_dim,\n",
        "]  \n",
        "## Repetitions of the cross-attention and Transformer modules.\n",
        "num_transformer_blocks = 4\n",
        "num_iterations = 2  \n",
        "# Size of the Feedforward network of the final classifier.\n",
        "classifier_units = [\n",
        "    projection_dim,\n",
        "    num_classes,\n",
        "]  \n",
        "\n",
        "##Cross checking \n",
        "print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n",
        "print(f\"Patches per image: {num_patches}\")\n",
        "print(f\"Elements per patch (3 channels): {(patch_size ** 2) * 3}\")\n",
        "print(f\"Latent array shape: {latent_dim} X {projection_dim}\")\n",
        "print(f\"Data array shape: {num_patches} X {projection_dim}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image size: 64 X 64 = 4096\n",
            "Patch size: 2 X 2 = 4 \n",
            "Patches per image: 1024\n",
            "Elements per patch (3 channels): 12\n",
            "Latent array shape: 256 X 256\n",
            "Data array shape: 1024 X 256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cx05krv8cuz-"
      },
      "source": [
        "## **Data Augmentation / Feature Engineering**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h02lZGZScrjS"
      },
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(image_size, image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxR1WRcic--E"
      },
      "source": [
        "## **Feedforward Network**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OjRzzdgdKmZ"
      },
      "source": [
        "def create_ffn(hidden_units, dropout_rate):\n",
        "    ffn_layers = []\n",
        "    for units in hidden_units[:-1]:\n",
        "        ffn_layers.append(layers.Dense(units, activation=tf.nn.gelu))\n",
        "\n",
        "    ffn_layers.append(layers.Dense(units=hidden_units[-1]))\n",
        "    ffn_layers.append(layers.Dropout(dropout_rate))\n",
        "\n",
        "    ffn = keras.Sequential(ffn_layers)\n",
        "    return ffn\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alP9rV0ldUD_"
      },
      "source": [
        "## **Implement patch creation as a layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtPomdmNdWih"
      },
      "source": [
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYXRlU9Ydqrc"
      },
      "source": [
        "## **Implement the patch encoding layer**\n",
        "The PatchEncoder layer will linearly transform a patch by projecting it into a vector of size latent_dim. In addition, it adds a learnable position embedding to the projected vector.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okox3oMfds2Q"
      },
      "source": [
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patches):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patches) + self.position_embedding(positions)\n",
        "        return encoded\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pnY9gE5d-Yr"
      },
      "source": [
        "## **Build the Perceiver model**\n",
        "Perceiver architecture is built from two components:\n",
        "* a cross-attention module that maps a byte array (e.g. an\n",
        "pixel array) and a latent array to a latent array, and \n",
        "* a Transformer tower that maps a latent array to a latent array\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QY-6LkgeHNT"
      },
      "source": [
        "### **Cross-attention module**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-kqBO-5dzXY"
      },
      "source": [
        "def create_cross_attention_module(\n",
        "    latent_dim, data_dim, projection_dim, ffn_units, dropout_rate\n",
        "):\n",
        "\n",
        "    inputs = {\n",
        "        # Recieve the latent array as an input of shape [1, latent_dim, projection_dim].\n",
        "        \"latent_array\": layers.Input(shape=(latent_dim, projection_dim)),\n",
        "        # Recieve the data_array (encoded image) as an input of shape [batch_size, data_dim, projection_dim].\n",
        "        \"data_array\": layers.Input(shape=(data_dim, projection_dim)),\n",
        "    }\n",
        "\n",
        "    # Apply layer norm to the inputs\n",
        "    latent_array = layers.LayerNormalization(epsilon=1e-6)(inputs[\"latent_array\"])\n",
        "    data_array = layers.LayerNormalization(epsilon=1e-6)(inputs[\"data_array\"])\n",
        "\n",
        "    # Create query tensor: [1, latent_dim, projection_dim].\n",
        "    query = layers.Dense(units=projection_dim)(latent_array)\n",
        "    # Create key tensor: [batch_size, data_dim, projection_dim].\n",
        "    key = layers.Dense(units=projection_dim)(data_array)\n",
        "    # Create value tensor: [batch_size, data_dim, projection_dim].\n",
        "    value = layers.Dense(units=projection_dim)(data_array)\n",
        "\n",
        "    # Generate cross-attention outputs: [batch_size, latent_dim, projection_dim].\n",
        "    attention_output = layers.Attention(use_scale=True, dropout=0.1)(\n",
        "        [query, key, value], return_attention_scores=False\n",
        "    )\n",
        "    # Skip connection 1.\n",
        "    attention_output = layers.Add()([attention_output, latent_array])\n",
        "\n",
        "    # Apply layer norm.\n",
        "    attention_output = layers.LayerNormalization(epsilon=1e-6)(attention_output)\n",
        "    # Apply Feedforward network.\n",
        "    ffn = create_ffn(hidden_units=ffn_units, dropout_rate=dropout_rate)\n",
        "    outputs = ffn(attention_output)\n",
        "    # Skip connection 2.\n",
        "    outputs = layers.Add()([outputs, attention_output])\n",
        "\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsA8w6qreV1Z"
      },
      "source": [
        "### **Transformer module**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBQHuwMteYHn"
      },
      "source": [
        "def create_transformer_module(\n",
        "    latent_dim,\n",
        "    projection_dim,\n",
        "    num_heads,\n",
        "    num_transformer_blocks,\n",
        "    ffn_units,\n",
        "    dropout_rate,\n",
        "):\n",
        "\n",
        "    # input_shape: [1, latent_dim, projection_dim]\n",
        "    inputs = layers.Input(shape=(latent_dim, projection_dim))\n",
        "\n",
        "    x0 = inputs\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        # Apply layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(x0)\n",
        "        # Create a multi-head self-attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, x0])\n",
        "        # Apply layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # Apply Feedforward network.\n",
        "        ffn = create_ffn(hidden_units=ffn_units, dropout_rate=dropout_rate)\n",
        "        x3 = ffn(x3)\n",
        "        # Skip connection 2.\n",
        "        x0 = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=x0)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKo-IvJfejwL"
      },
      "source": [
        "### **Perceiver model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtLnuAIDelgO"
      },
      "source": [
        "class Perceiver(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        patch_size,\n",
        "        data_dim,\n",
        "        latent_dim,\n",
        "        projection_dim,\n",
        "        num_heads,\n",
        "        num_transformer_blocks,\n",
        "        ffn_units,\n",
        "        dropout_rate,\n",
        "        num_iterations,\n",
        "        classifier_units,\n",
        "    ):\n",
        "        super(Perceiver, self).__init__()\n",
        "\n",
        "        self.latent_dim = latent_dim\n",
        "        self.data_dim = data_dim\n",
        "        self.patch_size = patch_size\n",
        "        self.projection_dim = projection_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.num_transformer_blocks = num_transformer_blocks\n",
        "        self.ffn_units = ffn_units\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.classifier_units = classifier_units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Create latent array.\n",
        "        self.latent_array = self.add_weight(\n",
        "            shape=(self.latent_dim, self.projection_dim),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "        # Create patching module.\n",
        "        self.patcher = Patches(self.patch_size)\n",
        "\n",
        "        # Create patch encoder.\n",
        "        self.patch_encoder = PatchEncoder(self.data_dim, self.projection_dim)\n",
        "\n",
        "        # Create cross-attention module.\n",
        "        self.cross_attention = create_cross_attention_module(\n",
        "            self.latent_dim,\n",
        "            self.data_dim,\n",
        "            self.projection_dim,\n",
        "            self.ffn_units,\n",
        "            self.dropout_rate,\n",
        "        )\n",
        "\n",
        "        # Create Transformer module.\n",
        "        self.transformer = create_transformer_module(\n",
        "            self.latent_dim,\n",
        "            self.projection_dim,\n",
        "            self.num_heads,\n",
        "            self.num_transformer_blocks,\n",
        "            self.ffn_units,\n",
        "            self.dropout_rate,\n",
        "        )\n",
        "\n",
        "        # Create global average pooling layer.\n",
        "        self.global_average_pooling = layers.GlobalAveragePooling1D()\n",
        "\n",
        "        # Create a classification head.\n",
        "        self.classification_head = create_ffn(\n",
        "            hidden_units=self.classifier_units, dropout_rate=self.dropout_rate\n",
        "        )\n",
        "\n",
        "        super(Perceiver, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Augment data.\n",
        "        augmented = data_augmentation(inputs)\n",
        "        # Create patches.\n",
        "        patches = self.patcher(augmented)\n",
        "        # Encode patches.\n",
        "        encoded_patches = self.patch_encoder(patches)\n",
        "        # Prepare cross-attention inputs.\n",
        "        cross_attention_inputs = {\n",
        "            \"latent_array\": tf.expand_dims(self.latent_array, 0),\n",
        "            \"data_array\": encoded_patches,\n",
        "        }\n",
        "        # Apply the cross-attention and the Transformer modules iteratively.\n",
        "        for _ in range(self.num_iterations):\n",
        "            # Apply cross-attention from the latent array to the data array.\n",
        "            latent_array = self.cross_attention(cross_attention_inputs)\n",
        "            # Apply self-attention Transformer to the latent array.\n",
        "            latent_array = self.transformer(latent_array)\n",
        "            # Set the latent array of the next iteration.\n",
        "            cross_attention_inputs[\"latent_array\"] = latent_array\n",
        "\n",
        "        # Apply global average pooling to generate a [batch_size, projection_dim] repesentation tensor.\n",
        "        representation = self.global_average_pooling(latent_array)\n",
        "        # Generate logits.\n",
        "        logits = self.classification_head(representation)\n",
        "        return logits\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCGqGjWIew_J"
      },
      "source": [
        "## **Compile, train, and evaluate the mode**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KLS6Woyeyxn"
      },
      "source": [
        "def run_experiment(model):\n",
        "\n",
        "    # Create LAMB optimizer with weight decay.\n",
        "    optimizer = tfa.optimizers.LAMB(\n",
        "        learning_rate=learning_rate, weight_decay_rate=weight_decay,\n",
        "    )\n",
        "\n",
        "    # Compile the model.\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # Create a learning rate scheduler callback.\n",
        "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\", factor=0.2, patience=3\n",
        "    )\n",
        "\n",
        "    # Create an early stopping callback.\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\", patience=15, restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    # Fit the model.\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[early_stopping, reduce_lr],\n",
        "    )\n",
        "\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    # Return history to plot learning curves.\n",
        "    return history\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3sTIq9Se8aS",
        "outputId": "03cb949c-251d-40c0-a791-da64c0f4c664"
      },
      "source": [
        "##Training the model\n",
        "perceiver_classifier = Perceiver(\n",
        "    patch_size,\n",
        "    num_patches,\n",
        "    latent_dim,\n",
        "    projection_dim,\n",
        "    num_heads,\n",
        "    num_transformer_blocks,\n",
        "    ffn_units,\n",
        "    dropout_rate,\n",
        "    num_iterations,\n",
        "    classifier_units,\n",
        ")\n",
        "\n",
        "\n",
        "history = run_experiment(perceiver_classifier)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "704/704 [==============================] - 557s 756ms/step - loss: 4.2831 - acc: 0.0606 - top5-acc: 0.1931 - val_loss: 3.9499 - val_acc: 0.0918 - val_top5-acc: 0.2848\n",
            "Epoch 2/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 3.9510 - acc: 0.1074 - top5-acc: 0.2979 - val_loss: 3.6652 - val_acc: 0.1392 - val_top5-acc: 0.3794\n",
            "Epoch 3/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 3.7131 - acc: 0.1458 - top5-acc: 0.3701 - val_loss: 3.3606 - val_acc: 0.1938 - val_top5-acc: 0.4628\n",
            "Epoch 4/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 3.4803 - acc: 0.1887 - top5-acc: 0.4380 - val_loss: 3.1248 - val_acc: 0.2334 - val_top5-acc: 0.5322\n",
            "Epoch 5/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 3.3095 - acc: 0.2243 - top5-acc: 0.4830 - val_loss: 3.0164 - val_acc: 0.2624 - val_top5-acc: 0.5598\n",
            "Epoch 6/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 3.1726 - acc: 0.2507 - top5-acc: 0.5189 - val_loss: 2.8719 - val_acc: 0.2804 - val_top5-acc: 0.5936\n",
            "Epoch 7/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 3.0600 - acc: 0.2757 - top5-acc: 0.5458 - val_loss: 2.7496 - val_acc: 0.3068 - val_top5-acc: 0.6206\n",
            "Epoch 8/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 2.9837 - acc: 0.2936 - top5-acc: 0.5629 - val_loss: 2.6691 - val_acc: 0.3232 - val_top5-acc: 0.6406\n",
            "Epoch 9/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 2.9008 - acc: 0.3080 - top5-acc: 0.5833 - val_loss: 2.6019 - val_acc: 0.3330 - val_top5-acc: 0.6578\n",
            "Epoch 10/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 2.8262 - acc: 0.3233 - top5-acc: 0.6014 - val_loss: 2.5291 - val_acc: 0.3498 - val_top5-acc: 0.6700\n",
            "Epoch 11/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 2.7652 - acc: 0.3382 - top5-acc: 0.6136 - val_loss: 2.4572 - val_acc: 0.3708 - val_top5-acc: 0.6916\n",
            "Epoch 12/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 2.7028 - acc: 0.3510 - top5-acc: 0.6292 - val_loss: 2.3736 - val_acc: 0.3854 - val_top5-acc: 0.7022\n",
            "Epoch 13/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 2.6372 - acc: 0.3656 - top5-acc: 0.6457 - val_loss: 2.3679 - val_acc: 0.3804 - val_top5-acc: 0.7038\n",
            "Epoch 14/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 2.5691 - acc: 0.3795 - top5-acc: 0.6611 - val_loss: 2.2842 - val_acc: 0.4064 - val_top5-acc: 0.7202\n",
            "Epoch 15/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 2.5367 - acc: 0.3895 - top5-acc: 0.6684 - val_loss: 2.2912 - val_acc: 0.4016 - val_top5-acc: 0.7130\n",
            "Epoch 16/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 2.4934 - acc: 0.3987 - top5-acc: 0.6804 - val_loss: 2.1915 - val_acc: 0.4298 - val_top5-acc: 0.7428\n",
            "Epoch 17/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 2.4374 - acc: 0.4084 - top5-acc: 0.6929 - val_loss: 2.1752 - val_acc: 0.4348 - val_top5-acc: 0.7372\n",
            "Epoch 18/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 2.3876 - acc: 0.4213 - top5-acc: 0.7037 - val_loss: 2.1800 - val_acc: 0.4362 - val_top5-acc: 0.7392\n",
            "Epoch 19/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 2.3486 - acc: 0.4284 - top5-acc: 0.7141 - val_loss: 2.1608 - val_acc: 0.4390 - val_top5-acc: 0.7468\n",
            "Epoch 20/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 2.3154 - acc: 0.4380 - top5-acc: 0.7212 - val_loss: 2.1536 - val_acc: 0.4408 - val_top5-acc: 0.7474\n",
            "Epoch 21/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 2.2693 - acc: 0.4484 - top5-acc: 0.7313 - val_loss: 2.1424 - val_acc: 0.4406 - val_top5-acc: 0.7534\n",
            "Epoch 22/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 2.2183 - acc: 0.4581 - top5-acc: 0.7411 - val_loss: 2.1433 - val_acc: 0.4466 - val_top5-acc: 0.7396\n",
            "Epoch 23/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 2.1928 - acc: 0.4651 - top5-acc: 0.7486 - val_loss: 2.0993 - val_acc: 0.4502 - val_top5-acc: 0.7532\n",
            "Epoch 24/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 2.1533 - acc: 0.4761 - top5-acc: 0.7584 - val_loss: 2.0587 - val_acc: 0.4626 - val_top5-acc: 0.7612\n",
            "Epoch 25/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 2.1171 - acc: 0.4823 - top5-acc: 0.7663 - val_loss: 2.0708 - val_acc: 0.4572 - val_top5-acc: 0.7558\n",
            "Epoch 26/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 2.0746 - acc: 0.4912 - top5-acc: 0.7763 - val_loss: 2.0652 - val_acc: 0.4664 - val_top5-acc: 0.7588\n",
            "Epoch 27/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 2.0457 - acc: 0.5001 - top5-acc: 0.7844 - val_loss: 2.0511 - val_acc: 0.4738 - val_top5-acc: 0.7620\n",
            "Epoch 28/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 1.9958 - acc: 0.5084 - top5-acc: 0.7962 - val_loss: 2.0437 - val_acc: 0.4698 - val_top5-acc: 0.7662\n",
            "Epoch 29/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 1.9750 - acc: 0.5157 - top5-acc: 0.7983 - val_loss: 2.0705 - val_acc: 0.4704 - val_top5-acc: 0.7630\n",
            "Epoch 30/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 1.9359 - acc: 0.5221 - top5-acc: 0.8074 - val_loss: 2.0469 - val_acc: 0.4802 - val_top5-acc: 0.7710\n",
            "Epoch 31/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 1.9128 - acc: 0.5307 - top5-acc: 0.8153 - val_loss: 2.0289 - val_acc: 0.4790 - val_top5-acc: 0.7712\n",
            "Epoch 32/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 1.8715 - acc: 0.5380 - top5-acc: 0.8244 - val_loss: 2.0640 - val_acc: 0.4734 - val_top5-acc: 0.7674\n",
            "Epoch 33/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 1.8338 - acc: 0.5482 - top5-acc: 0.8302 - val_loss: 2.0785 - val_acc: 0.4726 - val_top5-acc: 0.7658\n",
            "Epoch 34/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 1.8042 - acc: 0.5545 - top5-acc: 0.8390 - val_loss: 2.0820 - val_acc: 0.4702 - val_top5-acc: 0.7666\n",
            "Epoch 35/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 1.5421 - acc: 0.6221 - top5-acc: 0.8799 - val_loss: 1.9570 - val_acc: 0.5030 - val_top5-acc: 0.7902\n",
            "Epoch 36/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 1.4732 - acc: 0.6396 - top5-acc: 0.8936 - val_loss: 1.9810 - val_acc: 0.5002 - val_top5-acc: 0.7920\n",
            "Epoch 37/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 1.4319 - acc: 0.6479 - top5-acc: 0.9027 - val_loss: 1.9805 - val_acc: 0.5034 - val_top5-acc: 0.7900\n",
            "Epoch 38/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 1.4008 - acc: 0.6550 - top5-acc: 0.9087 - val_loss: 2.0069 - val_acc: 0.5052 - val_top5-acc: 0.7898\n",
            "Epoch 39/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 1.3421 - acc: 0.6705 - top5-acc: 0.9173 - val_loss: 2.0033 - val_acc: 0.5088 - val_top5-acc: 0.7912\n",
            "Epoch 40/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 1.3197 - acc: 0.6748 - top5-acc: 0.9206 - val_loss: 2.0142 - val_acc: 0.5014 - val_top5-acc: 0.7880\n",
            "Epoch 41/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 1.3084 - acc: 0.6804 - top5-acc: 0.9231 - val_loss: 2.0186 - val_acc: 0.5070 - val_top5-acc: 0.7928\n",
            "Epoch 42/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 1.2989 - acc: 0.6825 - top5-acc: 0.9242 - val_loss: 2.0217 - val_acc: 0.5058 - val_top5-acc: 0.7914\n",
            "Epoch 43/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 1.2936 - acc: 0.6835 - top5-acc: 0.9256 - val_loss: 2.0248 - val_acc: 0.5036 - val_top5-acc: 0.7916\n",
            "Epoch 44/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 1.2966 - acc: 0.6830 - top5-acc: 0.9265 - val_loss: 2.0253 - val_acc: 0.5056 - val_top5-acc: 0.7910\n",
            "Epoch 45/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 1.2942 - acc: 0.6839 - top5-acc: 0.9257 - val_loss: 2.0251 - val_acc: 0.5064 - val_top5-acc: 0.7906\n",
            "Epoch 46/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 1.2802 - acc: 0.6862 - top5-acc: 0.9268 - val_loss: 2.0250 - val_acc: 0.5050 - val_top5-acc: 0.7906\n",
            "Epoch 47/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 1.2917 - acc: 0.6848 - top5-acc: 0.9260 - val_loss: 2.0248 - val_acc: 0.5058 - val_top5-acc: 0.7906\n",
            "Epoch 48/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 1.2892 - acc: 0.6851 - top5-acc: 0.9267 - val_loss: 2.0247 - val_acc: 0.5058 - val_top5-acc: 0.7904\n",
            "Epoch 49/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 1.2753 - acc: 0.6890 - top5-acc: 0.9266 - val_loss: 2.0248 - val_acc: 0.5060 - val_top5-acc: 0.7906\n",
            "Epoch 50/50\n",
            "704/704 [==============================] - 531s 754ms/step - loss: 1.2882 - acc: 0.6857 - top5-acc: 0.9269 - val_loss: 2.0251 - val_acc: 0.5054 - val_top5-acc: 0.7912\n",
            "313/313 [==============================] - 43s 138ms/step - loss: 1.9144 - acc: 0.5162 - top5-acc: 0.7875\n",
            "Test accuracy: 51.62%\n",
            "Test top 5 accuracy: 78.75%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24bmJyrM8AZZ"
      },
      "source": [
        "After 35 epochs, the Perceiver model achieves around 50% accuracy on the test data.\n",
        "\n"
      ]
    }
  ]
}