{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2_PerceiverIO_MLM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_IzG6WjiS04"
      },
      "source": [
        "# **Perceiver IO model implementation**\n",
        "A General Architecture for Structured Inputs & Outputs by Deepmind\n",
        "* It is considered as a generalized architecture that solves quadratic time and space complexity with transformers which occurs due to attention mechanism.\n",
        "* It is a generalization of Perceiver to handle arbitrary outputs in addition to arbitrary inputs. \n",
        "* The original Perceiver only produced a single classification label. \n",
        "* In addition to classification labels, Perceiver IO can produce (for example) language, optical flow, and multimodal videos with audio. \n",
        "* This is done using the same building blocks as the original Perceiver. \n",
        "* The computational complexity of Perceiver IO is linear in the input and output size and the bulk of the processing occurs in the latent space, allowing us to process inputs and outputs that are much larger than can be handled by standard Transformers. \n",
        "* This means, for example, Perceiver IO can do BERT-style masked language modeling directly using bytes instead of tokenized inputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXb4_7wfXzyT"
      },
      "source": [
        "# **Masked language modeling**\n",
        "It involves: input a sentence into the model and optimizing the weights inside model to output the same sentence on the other side.\n",
        "* Normally BERT can be best suited for tasks such as MLM, this notebook demonstrates using perceiver IO for same and achieves the goal.\n",
        "\n",
        "## **References:**\n",
        "* https://github.com/2796gaurav/code_examples/blob/main/Perceiver/Perceiver_masked_language_modelling.ipynb\n",
        "* https://medium.com/analytics-vidhya/perceiver-io-a-general-architecture-for-structured-inputs-outputs-4ad669315e7f\n",
        "* https://deepmind.com/research/open-source/perceiver-IO\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeE4kR-PiOXe",
        "outputId": "095bd89f-b733-4dde-d966-c933f8f9ec14"
      },
      "source": [
        "# Install dependencies for Google Colab.\n",
        "# If you want to run this notebook on your own machine, you can skip this cell\n",
        "!pip install dm-haiku\n",
        "!pip install einops\n",
        "\n",
        "!mkdir /content/perceiver\n",
        "!touch /content/perceiver/__init__.py\n",
        "!wget -O /content/perceiver/bytes_tokenizer.py https://raw.githubusercontent.com/deepmind/deepmind-research/master/perceiver/bytes_tokenizer.py\n",
        "!wget -O /content/perceiver/io_processors.py https://raw.githubusercontent.com/deepmind/deepmind-research/master/perceiver/io_processors.py\n",
        "!wget -O /content/perceiver/perceiver.py https://raw.githubusercontent.com/deepmind/deepmind-research/master/perceiver/perceiver.py\n",
        "!wget -O /content/perceiver/position_encoding.py https://raw.githubusercontent.com/deepmind/deepmind-research/master/perceiver/position_encoding.py"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dm-haiku\n",
            "  Downloading dm_haiku-0.0.4-py3-none-any.whl (284 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▏                              | 10 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 30 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 40 kB 10.3 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 51 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████                         | 61 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████                        | 71 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 81 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 92 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 174 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 184 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 194 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 204 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 215 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 225 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 235 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 245 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 256 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 266 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 276 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 284 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from dm-haiku) (3.7.4.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from dm-haiku) (0.8.9)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from dm-haiku) (1.19.5)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from dm-haiku) (0.12.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.7.1->dm-haiku) (1.15.0)\n",
            "Installing collected packages: dm-haiku\n",
            "Successfully installed dm-haiku-0.0.4\n",
            "Collecting einops\n",
            "  Downloading einops-0.3.2-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.3.2\n",
            "--2021-10-10 22:39:26--  https://raw.githubusercontent.com/deepmind/deepmind-research/master/perceiver/bytes_tokenizer.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1850 (1.8K) [text/plain]\n",
            "Saving to: ‘/content/perceiver/bytes_tokenizer.py’\n",
            "\n",
            "/content/perceiver/ 100%[===================>]   1.81K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-10-10 22:39:26 (17.9 MB/s) - ‘/content/perceiver/bytes_tokenizer.py’ saved [1850/1850]\n",
            "\n",
            "--2021-10-10 22:39:26--  https://raw.githubusercontent.com/deepmind/deepmind-research/master/perceiver/io_processors.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29359 (29K) [text/plain]\n",
            "Saving to: ‘/content/perceiver/io_processors.py’\n",
            "\n",
            "/content/perceiver/ 100%[===================>]  28.67K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2021-10-10 22:39:26 (11.5 MB/s) - ‘/content/perceiver/io_processors.py’ saved [29359/29359]\n",
            "\n",
            "--2021-10-10 22:39:26--  https://raw.githubusercontent.com/deepmind/deepmind-research/master/perceiver/perceiver.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30179 (29K) [text/plain]\n",
            "Saving to: ‘/content/perceiver/perceiver.py’\n",
            "\n",
            "/content/perceiver/ 100%[===================>]  29.47K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2021-10-10 22:39:26 (12.4 MB/s) - ‘/content/perceiver/perceiver.py’ saved [30179/30179]\n",
            "\n",
            "--2021-10-10 22:39:26--  https://raw.githubusercontent.com/deepmind/deepmind-research/master/perceiver/position_encoding.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8107 (7.9K) [text/plain]\n",
            "Saving to: ‘/content/perceiver/position_encoding.py’\n",
            "\n",
            "/content/perceiver/ 100%[===================>]   7.92K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-10-10 22:39:27 (65.7 MB/s) - ‘/content/perceiver/position_encoding.py’ saved [8107/8107]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZuRrUQeXE6e"
      },
      "source": [
        "# Imports\n",
        "from typing import Union\n",
        "\n",
        "import haiku as hk\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "from perceiver import perceiver, position_encoding, io_processors, bytes_tokenizer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvd_Cc0ffvpX"
      },
      "source": [
        "## **Loading parameters from checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_F-LeOqXI_h",
        "outputId": "b99e21ed-5546-4bcc-d1a9-6c80328c34e6"
      },
      "source": [
        "## loading the pickle file\n",
        "!wget -O language_perceiver_io_bytes.pickle https://storage.googleapis.com/perceiver_io/language_perceiver_io_bytes.pickle\n",
        "\n",
        "with open(\"language_perceiver_io_bytes.pickle\", \"rb\") as f:\n",
        "  params = pickle.loads(f.read())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-10 22:39:59--  https://storage.googleapis.com/perceiver_io/language_perceiver_io_bytes.pickle\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.211.128, 173.194.213.128, 173.194.214.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.211.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 804479532 (767M) [application/octet-stream]\n",
            "Saving to: ‘language_perceiver_io_bytes.pickle’\n",
            "\n",
            "language_perceiver_ 100%[===================>] 767.21M   145MB/s    in 5.5s    \n",
            "\n",
            "2021-10-10 22:40:05 (139 MB/s) - ‘language_perceiver_io_bytes.pickle’ saved [804479532/804479532]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8iR7k3gf7EC"
      },
      "source": [
        "## **Model configuration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Qrr0egnXOO1"
      },
      "source": [
        "## Contruction the model with proper hyperparameters\n",
        "D_MODEL = 768\n",
        "D_LATENTS = 1280\n",
        "MAX_SEQ_LEN = 2048\n",
        "\n",
        "encoder_config = dict(\n",
        "    num_self_attends_per_block=26,\n",
        "    num_blocks=1,\n",
        "    z_index_dim=256,\n",
        "    num_z_channels=D_LATENTS,\n",
        "    num_self_attend_heads=8,\n",
        "    num_cross_attend_heads=8,\n",
        "    qk_channels=8 * 32,\n",
        "    v_channels=D_LATENTS,\n",
        "    use_query_residual=True,\n",
        "    cross_attend_widening_factor=1,\n",
        "    self_attend_widening_factor=1)\n",
        "\n",
        "decoder_config = dict(\n",
        "    output_num_channels=D_LATENTS,\n",
        "    position_encoding_type='trainable',\n",
        "    output_index_dims=MAX_SEQ_LEN,\n",
        "    num_z_channels=D_LATENTS,\n",
        "    qk_channels=8 * 32,\n",
        "    v_channels=D_MODEL,\n",
        "    num_heads=8,\n",
        "    final_project=False,\n",
        "    use_query_residual=False,\n",
        "    trainable_position_encoding_kwargs=dict(num_channels=D_MODEL))\n",
        "\n",
        "# The tokenizer is just UTF-8 encoding (with an offset)\n",
        "tokenizer = bytes_tokenizer.BytesTokenizer()\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Thheu1VgLJo"
      },
      "source": [
        "* Runs a forward pass on the Perceiver.\n",
        "\n",
        "* Args: inputs: input bytes, an int array of shape [B, T]\n",
        "* input_mask: Array of shape indicating which entries are valid and which are masked. A truthy value indicates that the entry is valid.\n",
        "* Returns: The output logits, an array of shape [B, T, vocab_size].\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8MAFQrIXS8V"
      },
      "source": [
        "## Decoding the Perceiver Model\n",
        "def apply_perceiver(\n",
        "    inputs: jnp.ndarray, input_mask: jnp.ndarray) -> jnp.ndarray:\n",
        "  \n",
        "  ##cross checking input size\n",
        "  assert inputs.shape[1] == MAX_SEQ_LEN\n",
        "\n",
        "  embedding_layer = hk.Embed(\n",
        "      vocab_size=tokenizer.vocab_size,\n",
        "      embed_dim=D_MODEL)\n",
        "  embedded_inputs = embedding_layer(inputs)\n",
        "\n",
        "  batch_size = embedded_inputs.shape[0]\n",
        "\n",
        "  input_pos_encoding = perceiver.position_encoding.TrainablePositionEncoding(\n",
        "      index_dim=MAX_SEQ_LEN, num_channels=D_MODEL)\n",
        "  embedded_inputs = embedded_inputs + input_pos_encoding(batch_size)\n",
        "  perceiver_mod = perceiver.Perceiver(\n",
        "      encoder=perceiver.PerceiverEncoder(**encoder_config),\n",
        "      decoder=perceiver.BasicDecoder(**decoder_config))\n",
        "  output_embeddings = perceiver_mod(\n",
        "      embedded_inputs, is_training=False, input_mask=input_mask, query_mask=input_mask)\n",
        "\n",
        "  logits = io_processors.EmbeddingDecoder(\n",
        "      embedding_matrix=embedding_layer.embeddings)(output_embeddings)\n",
        "  return logits\n",
        "\n",
        "apply_perceiver = hk.transform(apply_perceiver).apply"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwpG1wOPXbGM",
        "outputId": "ab08702c-9171-4a25-a249-5639ad39075e"
      },
      "source": [
        "input_str = \"This is an incomplete sentence where some words are missing.\"\n",
        "input_tokens = tokenizer.to_int(input_str)\n",
        "\n",
        "# Masking \"missing\". The model performs much better if the masked chunk starts with a space.\n",
        "input_tokens[51:60] = tokenizer.mask_token\n",
        "print(\"Tokenized string without masked bytes:\")\n",
        "print(tokenizer.to_string(input_tokens))\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized string without masked bytes:\n",
            "This is an incomplete sentence where some words are\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2B15lNVQXcfw"
      },
      "source": [
        "# Padding and reshaping inputs\n",
        "inputs = input_tokens[None]\n",
        "input_mask = np.ones_like(inputs)\n",
        "\n",
        "def pad(max_sequence_length: int, inputs, input_mask):\n",
        "  input_len = inputs.shape[1]\n",
        "  assert input_len <= max_sequence_length\n",
        "  pad_len = max_sequence_length - input_len\n",
        "  padded_inputs = np.pad(\n",
        "      inputs,\n",
        "      pad_width=((0, 0), (0, pad_len)),\n",
        "      constant_values=tokenizer.pad_token)\n",
        "  padded_mask = np.pad(\n",
        "      input_mask,\n",
        "      pad_width=((0, 0), (0, pad_len)),\n",
        "      constant_values=0)\n",
        "  return padded_inputs, padded_mask\n",
        "\n",
        "inputs, input_mask = pad(MAX_SEQ_LEN, inputs, input_mask)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eE5IxNmLXteK",
        "outputId": "45df157c-27f2-4ad8-e008-627f21bcd676"
      },
      "source": [
        "rng = jax.random.PRNGKey(1)  # Unused\n",
        "\n",
        "out = apply_perceiver(params, rng=rng, inputs=inputs, input_mask=input_mask)\n",
        "\n",
        "masked_tokens_predictions = out[0, 51:60].argmax(axis=-1)\n",
        "print(\"Greedy predictions:\")\n",
        "print(masked_tokens_predictions)\n",
        "print()\n",
        "print(\"Predicted string:\")\n",
        "print(tokenizer.to_string(masked_tokens_predictions))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greedy predictions:\n",
            "[ 38 115 111 121 121 111 116 109  52]\n",
            "\n",
            "Predicted string:\n",
            " missing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDGUiI9dirrf"
      },
      "source": [
        "***We can see that the model predicted the missing word to be 'missing'.***\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZTLznH2hivi",
        "outputId": "7c904ac7-5e56-4d4f-e60b-33de28795b60"
      },
      "source": [
        "input_str = \"I love eating chocolates\"\n",
        "input_tokens = tokenizer.to_int(input_str)\n",
        "\n",
        "# Masking \"chocolates\". The model performs much better if the masked chunk starts with a space.\n",
        "input_tokens[14:24] = tokenizer.mask_token\n",
        "print(\"Tokenized string without masked bytes:\")\n",
        "print(tokenizer.to_string(input_tokens))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized string without masked bytes:\n",
            "I love eating \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmA8xpgrhzlC",
        "outputId": "239b7822-0a8c-49ef-d4b2-c59cd1b9c0df"
      },
      "source": [
        "rng = jax.random.PRNGKey(1)  # Unused\n",
        "\n",
        "out = apply_perceiver(params, rng=rng, inputs=inputs, input_mask=input_mask)\n",
        "\n",
        "masked_tokens_predictions = out[0, 14:24].argmax(axis=-1)\n",
        "print(\"Greedy predictions:\")\n",
        "print(masked_tokens_predictions)\n",
        "print()\n",
        "print(\"Predicted string:\")\n",
        "print(tokenizer.to_string(masked_tokens_predictions))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greedy predictions:\n",
            "[38 38 38 38 38 38 38 38 38 38]\n",
            "\n",
            "Predicted string:\n",
            "          \n"
          ]
        }
      ]
    }
  ]
}